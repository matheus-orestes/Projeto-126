{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUWO5QkC_g-4"
      },
      "source": [
        "O **Tiro com Arco** é um jogo em que os jogadores atiram flechas de ponta afiada em um alvo redondo com 10 anéis.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QtHLAqv3wP3"
      },
      "source": [
        "## Problema de Aprendizado por Reforço (RL) a Resolver\n",
        "Acerte o centro do alvo que proporciona a recompensa máxima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osb6FQ74YZtE"
      },
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Número de **Estados**: ?\n",
        "\n",
        "Número de **Ações**: ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "outputs": [],
      "source": [
        "#Importe as bibliotecas\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      },
      "source": [
        "## Matriz de Recompensas\n",
        "A Matriz de Recompensas representa os estados como linhas e as ações como colunas com os respectivos valores de recompensas atribuídos a um determinado par de estado e ação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqPgOl0eh2u"
      },
      "outputs": [],
      "source": [
        "#Crie a matriz de recompensas\n",
        "rewards = np.array([\n",
        "    [15],\n",
        "    [25],\n",
        "    [50],\n",
        "    [75],\n",
        "    [100],\n",
        "    [100],\n",
        "    [75],\n",
        "    [50],\n",
        "    [25],\n",
        "    [15]\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-CAmdfkDQY"
      },
      "source": [
        "## Execute Ações Aleatoriamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSLCyMyigmK"
      },
      "outputs": [],
      "source": [
        "#Defina shoot()\n",
        " current_state = 3\n",
        " shoot = get_action(current_state, rewards)\n",
        " print(shoot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXKyVT28hHoH"
      },
      "source": [
        "##Matriz Q\n",
        "O **Aprendizado Q** é um algoritmo de aprendizado por reforço. Dado o estado atual, ele ajuda a encontrar a melhor ação a ser tomada pelo agente.\n",
        "\n",
        "A **Matriz Q** representa a recompensa recebida após uma ação específica no estado atual. Inicialmente, todos os elementos da matriz Q estão zerados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYwOV7ogtw1"
      },
      "outputs": [],
      "source": [
        "#Crie a matriz Q\n",
        "def create_matrix (n_linhas, n_colunas):\n",
        "    matrix = []\n",
        "    matrix.append(\" \")\n",
        "    matrix.append([\" \"]*n_linhas)\n",
        "\n",
        "    i = 0\n",
        "    while i < len(matrix):\n",
        "        matrix[i].append(\" \")\n",
        "        i += 1\n",
        "\n",
        "        return matrix\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c95A4SOkGdN"
      },
      "source": [
        "##Execute uma Ação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "outputs": [],
      "source": [
        "#Defina take_action\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "  \n",
        "     #Chame a função shoot() para obter a ação\n",
        "     shoot = getAction(current_state)\n",
        "\n",
        "     #Defina shoot()\n",
        "     current_state = 3\n",
        "\n",
        "     #Imprima a ação\n",
        "     print(shoot)\n",
        "\n",
        "     #Obtenha a recompensa correspondente usando a matriz de recompensas\n",
        "     shoot = get_action(rewards)\n",
        "\n",
        "     #Imprima a recompensa\n",
        "     print(reward)\n",
        "\n",
        "     return action, reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKy1VkgO4ZhP"
      },
      "source": [
        "## Atualize a Matriz Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U6NFICkhGMF"
      },
      "outputs": [],
      "source": [
        "#Defina o método run_episode()\n",
        "\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "\n",
        "  score = 0\n",
        "\n",
        " \n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5pcUO-5pJ-h"
      },
      "outputs": [],
      "source": [
        "#Chame o método run_episode para verificar a matriz Q final de um episódio\n",
        "run_episode(matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVY734TlZBx"
      },
      "source": [
        "## Treinar\n",
        "\n",
        "Crie uma função que execute um número de jogos, execute cada jogo por um determinado número de vezes e calcule as recompensas médias para cada vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smfOXGYZlp7b"
      },
      "outputs": [],
      "source": [
        "#Defina a função train()\n",
        "\n",
        "def train(episodes):\n",
        "  #Use o loop for para percorrer os episódios\n",
        "  for j in Q_matrix:\n",
        "    run_espisodes(episodes)\n",
        "\n",
        "    reward = 0 \n",
        "\n",
        "    run_episodes()\n",
        "\n",
        "    reward = episodes * episodes\n",
        "\n",
        "    \n",
        "  #retorne total_reward\n",
        "train = return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WYH_ioykcBC"
      },
      "source": [
        "##Treine 1000 episódios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbjnp3PSovsE"
      },
      "outputs": [],
      "source": [
        "#Execute a função train() para 1000 episódios\n",
        "train = episodes\n",
        "episodes = 1000\n",
        "train() = return episodes\n",
        "\n",
        "#Imprima a recompensa média\n",
        "return reward\n",
        "reward = episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jBnwogyuJP0"
      },
      "source": [
        "## Conclusão: \n",
        "\n",
        "Isso nos dá uma boa ideia sobre o desempenho geral do aprendizado de reforço mais simples com o problema **um estado - múltiplas ações**, também conhecido como problema \"**K-Armed Bandit**\".\n",
        "\n",
        "Um dos principais casos de uso desse tipo de problema pode ser visto na seleção do anúncio certo entre muitos a serem exibidos na página web. A máquina pode ser ensinada a escolher o melhor anúncio com mais cliques do usuário!!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PRO-C126-Project-Boilerplate-Code(pt).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
